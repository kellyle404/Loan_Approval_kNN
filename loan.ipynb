{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Untitled-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan approval prediction using KNN model (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.computersciencejournals.com/ijecs/article/view/30/1-2-18\n",
    "\n",
    "data: https://drive.google.com/file/d/1LIvIdqdHDFEGnfzIgEh4L6GFirzsE3US/view\n",
    "\n",
    "https://www.geeksforgeeks.org/loan-approval-prediction-using-machine-learning/\n",
    "\n",
    "for r: https://www.datacamp.com/tutorial/k-nearest-neighbors-knn-classification-with-r-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[**1. Introduction**](#introduction)\n",
    "\n",
    "[**2. Loading data**](#loading_data)\n",
    "* [2.1. Importing libraries](#libraries)\n",
    "* [2.2. Loading stock data](#stock)\n",
    "\n",
    "[**3. Basic analysis of stock information**](#racial_diversity)\n",
    "* [3.1. Closing price](#closing_price)\n",
    "* [3.2. Trading volume](#trading_volume)\n",
    "* [3.2. Moving average](#moving_average)\n",
    "* [3.3. Daily returns](#daily_returns)\n",
    "\n",
    "\n",
    "[**4. Risk analysis - Value at Risk**](#var)\n",
    "* [4.1. Historical approach](#historical)\n",
    "* [4.2. Parametric approach](#parametric)\n",
    "* [4.3. Monte Carlo approach](#montecarlo)\n",
    "\n",
    "[**5. Recommendations for future work**](#recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This project has 3 primary objectives:\n",
    "1. **Exploratory data analysis**: Conduct a thorough analysis of real-time financial data to identify patterns and trends.\n",
    "2. **Data visualisation**: Utilise visualisation techniques to present stock information effectively, aiding in the interpretation of market dynamics.\n",
    "3. **Predictive modelling**: Implement models to forecast stock movements and calculate Value at Risk (VaR).\n",
    "\n",
    "### Scope\n",
    "This project focuses on examining 4 prominent technology stocks i.e. Apple Inc. (AAPL), Amazon.com Inc. (AMZN), Alphabet Inc. (GOOG), and Microsoft Corporation (MSFT). The analysis spans a one-year period, from 28th January, 2023, to 28th January, 2024, covering 252 trading days.\n",
    "\n",
    "### Inquiry questions\n",
    "\n",
    "* How has the stock price changed over time?\n",
    "* What is the average daily return of the stock?\n",
    "* What is the moving average of the selected stocks?\n",
    "* What is the correlation between closing prices of different stocks?\n",
    "* What is the correlation between daily returns of different stocks?\n",
    "* How much value is at risk by investing in a particular stock?\n",
    "\n",
    "### Project outline\n",
    "\n",
    "The project begins with importing libraries and loading stock data for the aforementioned companies (Section 2). Then in Section 3, we will be conducting some basic analysis of the stock performance,  covering closing prices, trading volumes, moving averages, and daily returns. Section 4 - 'Risk Analysis - Value at Risk' - explores historical, parametric, and Monte Carlo approaches to computing a stock's Value at Risk. The project concludes with some potential areas for improvement in future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\tLoan\tA unique id \n",
    "2\tGender\tGender of the applicant Male/female\n",
    "3\tMarried\tMarital Status of the applicant, values will be Yes/ No\n",
    "4\tDependents\tIt tells whether the applicant has any dependents or not.\n",
    "5\tEducation\tIt will tell us whether the applicant is Graduated or not.\n",
    "6\tSelf_Employed\tThis defines that the applicant is self-employed i.e. Yes/ No\n",
    "7\tApplicantIncome\tApplicant income\n",
    "8\tCoapplicantIncome\tCo-applicant income\n",
    "9\tLoanAmount\tLoan amount (in thousands)\n",
    "10\tLoan_Amount_Term\tTerms of loan (in months)\n",
    "11\tCredit_History\tCredit history of individualâ€™s repayment of their debts\n",
    "12\tProperty_Area\tArea of property i.e. Rural/Urban/Semi-urban \n",
    "13\tLoan_Status\tStatus of Loan Approved or not i.e. Y- Yes, N-No "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n",
      "File \u001b[1;32mc:\\Users\\kalul\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     ArrowDtype,\n\u001b[0;32m     62\u001b[0m     Int8Dtype,\n\u001b[0;32m     63\u001b[0m     Int16Dtype,\n\u001b[0;32m     64\u001b[0m     Int32Dtype,\n\u001b[0;32m     65\u001b[0m     Int64Dtype,\n\u001b[0;32m     66\u001b[0m     UInt8Dtype,\n\u001b[0;32m     67\u001b[0m     UInt16Dtype,\n\u001b[0;32m     68\u001b[0m     UInt32Dtype,\n\u001b[0;32m     69\u001b[0m     UInt64Dtype,\n\u001b[0;32m     70\u001b[0m     Float32Dtype,\n\u001b[0;32m     71\u001b[0m     Float64Dtype,\n\u001b[0;32m     72\u001b[0m     CategoricalDtype,\n\u001b[0;32m     73\u001b[0m     PeriodDtype,\n\u001b[0;32m     74\u001b[0m     IntervalDtype,\n\u001b[0;32m     75\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     76\u001b[0m     StringDtype,\n\u001b[0;32m     77\u001b[0m     BooleanDtype,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     NA,\n\u001b[0;32m     80\u001b[0m     isna,\n\u001b[0;32m     81\u001b[0m     isnull,\n\u001b[0;32m     82\u001b[0m     notna,\n\u001b[0;32m     83\u001b[0m     notnull,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     Index,\n\u001b[0;32m     86\u001b[0m     CategoricalIndex,\n\u001b[0;32m     87\u001b[0m     RangeIndex,\n\u001b[0;32m     88\u001b[0m     MultiIndex,\n\u001b[0;32m     89\u001b[0m     IntervalIndex,\n\u001b[0;32m     90\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     91\u001b[0m     DatetimeIndex,\n\u001b[0;32m     92\u001b[0m     PeriodIndex,\n\u001b[0;32m     93\u001b[0m     IndexSlice,\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     NaT,\n\u001b[0;32m     96\u001b[0m     Period,\n\u001b[0;32m     97\u001b[0m     period_range,\n\u001b[0;32m     98\u001b[0m     Timedelta,\n\u001b[0;32m     99\u001b[0m     timedelta_range,\n\u001b[0;32m    100\u001b[0m     Timestamp,\n\u001b[0;32m    101\u001b[0m     date_range,\n\u001b[0;32m    102\u001b[0m     bdate_range,\n\u001b[0;32m    103\u001b[0m     Interval,\n\u001b[0;32m    104\u001b[0m     interval_range,\n\u001b[0;32m    105\u001b[0m     DateOffset,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     to_numeric,\n\u001b[0;32m    108\u001b[0m     to_datetime,\n\u001b[0;32m    109\u001b[0m     to_timedelta,\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     Flags,\n\u001b[0;32m    112\u001b[0m     Grouper,\n\u001b[0;32m    113\u001b[0m     factorize,\n\u001b[0;32m    114\u001b[0m     unique,\n\u001b[0;32m    115\u001b[0m     value_counts,\n\u001b[0;32m    116\u001b[0m     NamedAgg,\n\u001b[0;32m    117\u001b[0m     array,\n\u001b[0;32m    118\u001b[0m     Categorical,\n\u001b[0;32m    119\u001b[0m     set_eng_float_format,\n\u001b[0;32m    120\u001b[0m     Series,\n\u001b[0;32m    121\u001b[0m     DataFrame,\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\kalul\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\api.py:16\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     isna,\n\u001b[0;32m     18\u001b[0m     isnull,\n\u001b[0;32m     19\u001b[0m     notna,\n\u001b[0;32m     20\u001b[0m     notnull,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     factorize,\n\u001b[0;32m     25\u001b[0m     unique,\n\u001b[0;32m     26\u001b[0m     value_counts,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n",
      "File \u001b[1;32mc:\\Users\\kalul\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\missing.py:181\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isna(obj)\n\u001b[1;32m--> 181\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    182\u001b[0m isnull \u001b[38;5;241m=\u001b[39m isna\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_isna\u001b[39m(obj, inf_as_na: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoanApprovalPrediction.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m display(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('LoanApprovalPrediction.csv')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of applicants: 598\n",
    "\n",
    "Total number of fields: 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation\n",
    "\n",
    "After finding about No. of blank fields present in the dataset then we must replace them with values which are derived by statistical methods such as mean, mode, mean for both numerical and categorical attributes present in the dataset and must check for null values to make sure that there are no blank fields in the dataset. We can also replace the irrelevant or noisy data with the precise ones so that it will not show any impact on the training process and to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Dependents'] = data['Dependents'].fillna(data['Dependents'].mean())\n",
    "data['LoanAmount'] = data['LoanAmount'].fillna(data['LoanAmount'].mean())\n",
    "data['Loan_Amount_Term'] = data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].mean())\n",
    "data['Credit_History'] = data['Credit_History'].fillna(data['Credit_History'].mean())\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Loan_ID is completely unique and not correlated with any of the other column, So we will drop it using .drop() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Loan_ID'],axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no missing value then we must proceed to model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we must divide the data into independent and dependent variables which means we must split first 12 attributes variables into one group of array elements and the final status attribute variables into other as they are dependent on the other attributes of the dataset.\n",
    "* x = predictor variables\n",
    "* y = response variable - loan status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the variables into two groups then we must transform all the categorical data variables into the machine understandable format. So that we will convert them into some dummy variables. Here we will use LabelEncoder( ), OneHotEncoder( ), fitTransform( ) functions for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status']\n",
    "\n",
    "# Initialising LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Predictor variables\n",
    "for col in categorical:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting all the categorical data into dummy variables and loading it into again the same variable â€˜Xâ€™, we must split both the data variables â€˜Xâ€™ and â€˜Yâ€™ into train and test data using train_test_split module available from scikit\u0002learn. Thereafter we must fit the split data using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    data,\n",
    "    test_size = 0.4, \n",
    "    random_state = 404)\n",
    "\n",
    "display(train.head(2))\n",
    "display(test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-NN\n",
    "\n",
    "finding the distance between a new point (Alice) and each point in the training sample\n",
    "\n",
    "sorting the data table by these distances\n",
    "\n",
    "selecting the top k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the distance between any 2 points\n",
    "def dist_pt_pt1(point, point1):\n",
    "    '''\n",
    "    Input: point & point 1 (each is an array consisting of the coordinates of the point)\n",
    "    Output:  distance between point and point 1\n",
    "    '''    \n",
    "    return np.sqrt(np.sum((point - point1)**2))\n",
    "\n",
    "# 2. Compute the distance between a point and every other point in the data set\n",
    "def dist_pt_other(point, train_dataset):\n",
    "    '''\n",
    "    Input: point 1 & point 2 (each is an array consisting of the coordinates of the point)\n",
    "    Output:  distance between point 1 and point 2\n",
    "    '''\n",
    "    predictor_var = train.drop(columns = 'Loan_Status').copy()\n",
    "    distance_column = predictor_var.apply(lambda row: dist_pt_pt1(point, row), axis = 1)\n",
    "    train_dataset['distance'] = distance_column\n",
    "    return train_dataset\n",
    "\n",
    "# 3. Pick out k nearest neighbour and identify the classification of the test point\n",
    "def knn(point, k, train_dataset):\n",
    "    '''\n",
    "    Input: point 1 & point 2 (each is an array consisting of the coordinates of the point)\n",
    "    Output:  distance between point 1 and point 2\n",
    "    '''\n",
    "    train_dataset = dist_pt_other(point, train_dataset)\n",
    "    train_dataset = train_dataset.sort_values(by = 'distance', ascending = True)\n",
    "    knn = train_dataset.head(k)\n",
    "    classification = knn['Loan_Status'].mode()\n",
    "    return classification.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['CoapplicantIncome', 'ApplicantIncome']\n",
    "train = data[['Loan_Status'] + attributes]\n",
    "\n",
    "knn_result = knn(train.iloc[4], 1, train)\n",
    "\n",
    "knn_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the accuracy of the knn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Msitake: when runnig  thr model on the train set the status clumn MUST BE REOMVED OHTERWISE ITS > 2 ATTRIBUTES -> DISTANCE BC SUPER LARGE -> WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_prediction = train.drop(columns = {'Loan_Status', 'distance'})\n",
    "knn_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_prediction = knn_prediction.apply(lambda row: knn(row, 1, train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runtime 3m 58.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_result = train.copy()\n",
    "knn_result['prediction'] = knn_prediction\n",
    "knn_result['correct'] = (knn_result['Loan_Status'] == knn_result['prediction'])\n",
    "\n",
    "display(knn_result)\n",
    "knn_result.value_counts('correct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_result[knn_result['correct'] == False]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
